{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone - Sentiment Based Product Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submitted By: Deepesh Dhapola"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The e-commerce business is quite popular today. Here, you do not need to take orders by going to each customer. A company launches its website to sell the items to the end consumer, and customers can order the products that they require from the same website. Famous examples of such e-commerce companies are Amazon, Flipkart, Myntra, Paytm and Snapdeal.\n",
    "\n",
    "Suppose you are working as a Machine Learning Engineer in an e-commerce company named 'Ebuss'. Ebuss has captured a huge market share in many fields, and it sells the products in various categories such as household essentials, books, personal care products, medicines, cosmetic items, beauty products, electrical appliances, kitchen and dining products and health care products.\n",
    "\n",
    "With the advancement in technology, it is imperative for Ebuss to grow quickly in the e-commerce market to become a major leader in the market because it has to compete with the likes of Amazon, Flipkart, etc., which are already market leaders.\n",
    "\n",
    "As a senior ML Engineer, you are asked to build a model that will improve the recommendations given to the users given their past reviews and ratings.\n",
    "\n",
    "In order to do this, you planned to build a sentiment-based product recommendation system, which includes the following tasks.\n",
    "\n",
    "Data sourcing and sentiment analysis Building a recommendation system Improving the recommendations using the sentiment analysis model Deploying the end-to-end project with a user interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general purpose libraries \n",
    "\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "import platform\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn import metrics\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.metrics.pairwise import pairwise_distances, cosine_similarity\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#from numpy import *\n",
    "\n",
    "# set options\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_colwidth', 300)\n",
    "pd.set_option(\"display.precision\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/a-9869/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/a-9869/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/a-9869/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/a-9869/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/a-9869/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# download and import NLTK libraries\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modelling libraries\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python and Python library versions in use\n",
      "Python: 3.9.10\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sklearn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPython and Python library versions in use\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPython: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m platform\u001b[38;5;241m.\u001b[39mpython_version())\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msklearn: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[43msklearn\u001b[49m\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpandas: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m pd\u001b[38;5;241m.\u001b[39m__version__)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sklearn' is not defined"
     ]
    }
   ],
   "source": [
    "# check library versions\n",
    "\n",
    "print('Python and Python library versions in use')\n",
    "print('Python: ' + platform.python_version())\n",
    "print('sklearn: ' + sklearn.__version__)\n",
    "print('numpy: ' + np.__version__)\n",
    "print('pandas: ' + pd.__version__)\n",
    "print('nltk: ' + nltk.__version__)\n",
    "print('re (regular expression): ' + re.__version__)\n",
    "print('sns: ' + sns.__version__)\n",
    "print('xgboost: ' + xgb.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Analyse Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dataset provided\n",
    "df_reviews = pd.read_csv(\"dataset/sample30.csv\")\n",
    "df_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis - Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_missing_rows(df):\n",
    "    # summing up the missing values (column-wise) and displaying fraction of NaNs\n",
    "    return df.isnull().sum()\n",
    "\n",
    "calc_missing_rows(df_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the missing row of user_sentiment\n",
    "df_reviews = df_reviews[~df_reviews.user_sentiment.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map the categorical user_sentiment to numerical 1 or 0 for modelling\n",
    "df_reviews['user_sentiment'] = df_reviews['user_sentiment'].map({'Positive':1,'Negative':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the value count of user_sentiments column\n",
    "df_reviews[\"user_sentiment\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize using count plot\n",
    "sns.countplot(x='user_sentiment', data= df_reviews, palette=\"Set2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see overrepresentation of positive reviews. we may have to do Class Imbalance techniques during modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews[\"reviews_rating\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews[\"reviews_rating\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize the user_rating..\n",
    "sns.countplot(x='reviews_rating', data= df_reviews, palette=\"Set2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews[df_reviews[\"user_sentiment\"]==1][\"reviews_rating\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews[df_reviews[\"user_sentiment\"]==0][\"reviews_rating\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews[(df_reviews[\"user_sentiment\"]==1) & (df_reviews[\"reviews_rating\"]<4)][[\"reviews_title\",\"reviews_text\", \"reviews_rating\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews[(df_reviews[\"user_sentiment\"]==0) & (df_reviews[\"reviews_rating\"]>=4)][[\"reviews_title\",\"reviews_text\", \"reviews_rating\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df_reviews[\"user_sentiment\"], df_reviews[\"reviews_rating\"], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per the above results, there looks to be mismtach between review_rating and user_sentiment. there are records that have higher user rating but user sentiment is negative and lower user rating but user sentiment is positive. We can either clean up those records or update the target column to the correct user sentiment, so that will be helpful for modelling. Here, updating the user_sentiment to 0 (but was 1) when the reviews_rating is less than 4, and updating the user_sentiment to 1(but was 0), when the reviews_rating is greater than or equal to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews.loc[(df_reviews[\"user_sentiment\"]==1) & (df_reviews[\"reviews_rating\"]<4), \"user_sentiment\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews.loc[(df_reviews[\"user_sentiment\"]==0) & (df_reviews[\"reviews_rating\"]>=4), \"user_sentiment\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df_reviews[\"user_sentiment\"], df_reviews[\"reviews_rating\"], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the above table, we can see that we don't have mismatch between the reviews_rating and user_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews[\"user_sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews[\"brand\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the top 10 brands among the positive sentiments\n",
    "df_reviews[df_reviews['user_sentiment']==1].groupby('brand')['brand'].count().sort_values(ascending=False)[:10].plot(kind='bar',color='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the top 10 brands among the negative sentiments\n",
    "df_reviews[df_reviews['user_sentiment']==0].groupby('brand')['brand'].count().sort_values(ascending=False)[:10].plot(kind='bar', color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_products(productId, pos=1):\n",
    "    review_count = df_reviews[(df_reviews.id==productId) & (df_reviews.user_sentiment==pos)]['brand'].count()\n",
    "    return review_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group the dataframe by product id and view the # of pos review / neg reviews count\n",
    "df_custom =  df_reviews.groupby('id', as_index=False)['user_sentiment'].count()\n",
    "df_custom[\"pos_review_count\"] =  df_custom.id.apply(lambda id: filter_products(id, 1))\n",
    "df_custom[\"neg_review_count\"] =  df_custom.id.apply(lambda id: filter_products(id, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_custom.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort the product by sentiment % - postive reviews / total number of reviews\n",
    "df_custom['sentiment %'] = np.round((df_custom['pos_review_count']/df_custom['user_sentiment'])*100,2)\n",
    "df_custom.sort_values(by='sentiment %', ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews[\"manufacturer\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's find out the most common users.\n",
    "df_reviews[\"reviews_username\"].value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the customers by 'positive user sentiment'\n",
    "df_reviews[df_reviews['user_sentiment']==1].groupby('reviews_username')['reviews_username'].count().sort_values(ascending=False)[:10].plot(kind='bar', color='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the customers by 'negative user sentiment'\n",
    "df_reviews[df_reviews['user_sentiment']==0].groupby('reviews_username')['reviews_username'].count().sort_values(ascending=False)[:10].plot(kind='bar', color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing nan/null from username\n",
    "df_reviews = df_reviews[~df_reviews.reviews_username.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's combine the reviews_text and reviews_title for better analysis\n",
    "df_reviews[\"reviews_title\"] = df_reviews[\"reviews_title\"].fillna('')\n",
    "df_reviews[\"reviews_full_text\"] = df_reviews[['reviews_title', 'reviews_text']].agg('. '.join, axis=1).str.lstrip('. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the missing row cout for each of the columns\n",
    "calc_missing_rows(df_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required columns for reviews_rating, reviews_text, user_sentiment,reviews_username doesn't have null/na values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text Preprocessing for Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews[[\"reviews_full_text\", \"user_sentiment\"]].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''function to clean the text and remove all the unnecessary elements.'''\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = re.sub(\"\\[\\s*\\w*\\s*\\]\", \"\", text)\n",
    "    dictionary = \"abc\".maketrans('', '', string.punctuation)\n",
    "    text = text.translate(dictionary)\n",
    "    text = re.sub(\"\\S*\\d\\S*\", \"\", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_reviews[['id','name', 'reviews_full_text', 'user_sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[\"reviews_text\"] = df_clean.reviews_full_text.apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a helper function to map NTLK position tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopword(text):\n",
    "    words = [word for word in text.split() if word.isalpha() and word not in stop_words]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "# Lemmatize the sentence\n",
    "def lemma_text(text):\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(remove_stopword(text))) # Get position tags\n",
    "    # Map the position tag and lemmatize the word/token\n",
    "    words =[lemmatizer.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] \n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[\"reviews_text_cleaned\"] = df_clean.reviews_text.apply(lambda x: lemma_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reviews_text is cleaned to remove stopwords, punctuations,numericals,whitespaces and lemma is derived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a word cloud find the top 40 words by frequency among all the reviews after processing the text\n",
    "\n",
    "\n",
    "wordcloud = WordCloud(stopwords=stop_words,max_words=200).generate(str(df_clean.reviews_text_cleaned))\n",
    "\n",
    "print(wordcloud)\n",
    "plt.figure(figsize= (10,10))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the data according to the 'Review Text' character length\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "reviews_lens = [len(d) for d in df_clean.reviews_text_cleaned]\n",
    "plt.hist(reviews_lens, bins = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find most common words in reviews\n",
    "\n",
    "def get_most_common_words(reviews, n_most_common):\n",
    "    # flatten review column into a list of words, and set each to lowercase\n",
    "    flattened_reviews = [word for review in reviews for word in \\\n",
    "                         review.lower().split()]\n",
    "\n",
    "\n",
    "    # remove punctuation from reviews\n",
    "    flattened_reviews = [''.join(char for char in review if \\\n",
    "                                 char not in string.punctuation) for \\\n",
    "                         review in flattened_reviews]\n",
    "\n",
    "\n",
    "    # remove any empty strings that were created by this process\n",
    "    flattened_reviews = [review for review in flattened_reviews if review]\n",
    "\n",
    "    return Counter(flattened_reviews).most_common(n_most_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pos_reviews = df_clean[df_clean['user_sentiment']==1]\n",
    "get_most_common_words(pos_reviews['reviews_text_cleaned'],10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_reviews = df_clean[df_clean['user_sentiment']==0]\n",
    "get_most_common_words(neg_reviews['reviews_text_cleaned'],10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to collect the n-gram frequency of words\n",
    "\n",
    "def get_top_n_ngram( corpus, n_gram_range ,n=None):\n",
    "    vec = CountVectorizer(ngram_range=(n_gram_range, n_gram_range), stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    #print(bag_of_words)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    print(\"--1\",sum_words)\n",
    "    for word, idx in vec.vocabulary_.items():\n",
    "        #print(word)\n",
    "        #print(idx)\n",
    "        break\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    #print(\"-31\",words_freq)\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the top 10 words in the bigram frequency\n",
    "\n",
    "common_words = get_top_n_ngram(pos_reviews['reviews_text_cleaned'], 2, 10)\n",
    "pd.DataFrame(common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the top 10 words in the bigram frequency\n",
    "\n",
    "common_words = get_top_n_ngram(neg_reviews['reviews_text_cleaned'], 2, 10)\n",
    "pd.DataFrame(common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the top 10 trigram frequency among the reviews_text in the cleaned datafram(df_clean). \n",
    "\n",
    "common_words = get_top_n_ngram(df_clean.reviews_text_cleaned, 3, 10)\n",
    "df3 = pd.DataFrame(common_words, columns = ['trigram' , 'count'])\n",
    "plt.figure(figsize=[35,25])\n",
    "fig = sns.barplot(x=df3['trigram'], y=df3['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_clean['reviews_text_cleaned']\n",
    "y = df_clean['user_sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_classes= len(pd.Series(y).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of the target variable data in terms of proportions.\n",
    "for i in range(0,no_of_classes):\n",
    "    print(\"Percent of {0}s: \".format(i), round(100*pd.Series(y).value_counts()[i]/pd.Series(y).value_counts().sum(),2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Vectorize the data, using TF-IDF vectorizer method to dervie the features from the textual data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it's clearly a class-imbalance between positive and negative, let's do SMOTE oversampling technique before modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using TF-IDF vectorizer using the parameters to get 650 features.\n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=650, max_df=0.9, min_df=7, binary=True, \n",
    "                                   ngram_range=(1,2))\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(df_clean['reviews_text_cleaned'])\n",
    "\n",
    "y= df_clean['user_sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into test and train\n",
    "\n",
    "\n",
    "from sklearn.model_selection  import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_tfidf, y, random_state=42, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class imbalance (using SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(y_train)\n",
    "print('Before',counter)\n",
    "\n",
    "sm = SMOTE()\n",
    "\n",
    "# transform the dataset\n",
    "X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "counter = Counter(y_train)\n",
    "print('After',counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Building - Training a text classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBuilder:\n",
    "    def __init__(self, model, x_train, x_test, y_train, y_test):\n",
    "        self.model = model\n",
    "        self.x_train = x_train\n",
    "        self.x_test = x_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        \n",
    "        \n",
    "    def train_model(self):\n",
    "        self.model.fit(self.x_train,self.y_train)\n",
    "        return self.model.predict(self.x_test)\n",
    "    \n",
    "    def evaluate_model(self, y_pred_class):\n",
    "        print(\"\\n\")\n",
    "        print(\"*\"*30)\n",
    "        self.result_metrics = self.evaluate_metrics(y_pred_class)\n",
    "        print(\"*\"*30)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        self.classification_report(y_pred_class)\n",
    "        print(\"*\"*30)\n",
    "        print(\"\\n\")\n",
    "        self.confusion_matrix(y_pred_class)\n",
    "            \n",
    "        print(\"*\"*30)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        metrics.plot_roc_curve(self.model, self.x_test, self.y_test)\n",
    "        \n",
    "        return self.result_metrics\n",
    "        \n",
    "    def evaluate_metrics(self, y_pred_class):\n",
    "        result_metrics = [] \n",
    "        accuracy = metrics.accuracy_score(self.y_test, y_pred_class)\n",
    "        precision = metrics.precision_score(self.y_test, y_pred_class)\n",
    "        recall = metrics.recall_score(self.y_test, y_pred_class)\n",
    "        f1score = metrics.f1_score(self.y_test, y_pred_class)\n",
    "        y_pred_prob = self.model.predict_proba(self.x_test)[:,1]\n",
    "        roc_auc = metrics.roc_auc_score(self.y_test, y_pred_prob)\n",
    "        \n",
    "        print(f\"Accuracy is : {accuracy*100:.1f}%\")\n",
    "        print(f\"Precision is : {precision*100:.1f}%\")\n",
    "        print(f\"Recall is : {recall*100:.1f}%\")\n",
    "        print(f\"F1 Score is : {f1score*100:.1f}%\")\n",
    "        print(f\"Roc-Auc Score is:{roc_auc*100:.1f}%\")\n",
    "        \n",
    "        result_metrics.append(accuracy)\n",
    "        result_metrics.append(precision)\n",
    "        result_metrics.append(recall)\n",
    "        result_metrics.append(f1score)\n",
    "        result_metrics.append(roc_auc)\n",
    "        return result_metrics\n",
    "        \n",
    "    def confusion_matrix(self, y_pred_class):\n",
    "        confusion_matrix = metrics.confusion_matrix(self.y_test, y_pred_class)\n",
    "        self.plot_confusion_matrix(confusion_matrix,[0,1])\n",
    "        \n",
    "        \n",
    "    def plot_confusion_matrix(self, data, labels):\n",
    "        sns.set(color_codes=True)\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        ax = sns.heatmap(data/np.sum(data), annot=True, cmap=\"Blues\", fmt=\".2%\")\n",
    " \n",
    "        ax.set_xticklabels(labels)\n",
    "        ax.set_yticklabels(labels)\n",
    " \n",
    "        ax.set(ylabel=\"True Values\", xlabel=\"Predicted Values\")\n",
    "        plt.show()\n",
    "        \n",
    "    def classification_report(self, y_pred_class):\n",
    "        print(metrics.classification_report(self.y_test, y_pred_class))\n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f1-score/recall score for class 0 is low and this could be due to class-imbalance, though the other metrics seem to be good. Let's use class-imbalance techniques using SMOTE and do the modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the Logistic Regression model.\n",
    "\n",
    "%time\n",
    "logreg_ci = LogisticRegression(random_state=42, max_iter=100,solver='liblinear', class_weight=\"balanced\")\n",
    "lr_ci_modebuilder = ModelBuilder(logreg_ci, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Predict the Test Labels\n",
    "y_pred_class  = lr_ci_modebuilder.train_model()\n",
    "lr_metrics = lr_ci_modebuilder.evaluate_model(y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 Score looks to be good, though the individual class(0) is lesser. let;s try with other alogithms if we can increase the overall F1 and for the individual classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the NB model and making predictions\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB(alpha=1.0)\n",
    "mnb_modebuilder = ModelBuilder(mnb, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Predict the Test Labels\n",
    "y_pred_class  = mnb_modebuilder.train_model()\n",
    "nb_metrics = mnb_modebuilder.evaluate_model(y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state=42, criterion=\"gini\", max_depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_modelbuilder = ModelBuilder(dt, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class  = dt_modelbuilder.train_model()\n",
    "dt_metrics_cv = dt_modelbuilder.evaluate_model(y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(oob_score=True, random_state=42, criterion=\"gini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'max_depth': [2,3,5,10],\n",
    "    'min_samples_leaf': [5,10,20,50],\n",
    "    'n_estimators': [10, 25, 50, 100]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator=rf,\n",
    "                           param_grid=params,\n",
    "                           cv = 4,\n",
    "                           n_jobs=-1, verbose=1, scoring=\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best = grid_search.best_estimator_\n",
    "rf_modebuilder = ModelBuilder(rf_best, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Predict the Test Labels\n",
    "y_pred_class  = rf_modebuilder.train_model()\n",
    "rf_metrics = rf_modebuilder.evaluate_model(y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgclf = xgb.XGBClassifier(learning_rate=0.15, max_depth=10, random_state=42) #based on the tuned parameters\n",
    "xg_modebuilder = ModelBuilder(xgclf, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Predict the Test Labels\n",
    "y_pred_class  = xg_modebuilder.train_model()\n",
    "xg_metrics = xg_modebuilder.evaluate_model(y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a table which contain all the metrics\n",
    "\n",
    "metrics_table = {'Metric': ['Accuracy','Precision','Recall',\n",
    "                       'F1Score','Auc Score'], \n",
    "        'Logistic Regression': lr_metrics,\n",
    "        'Naive Bayes': nb_metrics,\n",
    "        'Decision Tree': dt_metrics_cv,\n",
    "         'Random Forrest': rf_metrics,\n",
    "        'XG Boost': xg_metrics\n",
    "        }\n",
    "\n",
    "df_metrics = pd.DataFrame(metrics_table ,columns = ['Metric', 'Logistic Regression', 'Naive Bayes','Decision Tree','Random Forrest',\n",
    "                                                    'XG Boost'] )\n",
    "\n",
    "df_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the above models on the metrics, XG Boost looks to be a better model, saving the XG Boost model as a pickle file for later use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_object(obj, filename):\n",
    "    filename = \"pickle/\"+filename+'.pkl'\n",
    "    pickle.dump(obj, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(xgclf, 'sentiment-classification-xg-boost-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(tfidf_vectorizer, 'tfidf-vectorizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(df_clean, 'cleaned-data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Building a recommendation system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different Approaches to develop Recommendation System -\n",
    "\n",
    "Here we are going to try with 2 of the Colloboarative filtering techniques:\n",
    "- User-User Based Approach\n",
    "- Item-Item Based Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recommendation = df_reviews[[\"id\", \"name\", \"reviews_rating\", \"reviews_username\"]]\n",
    "calc_missing_rows(df_recommendation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the train and test\n",
    "train, test = train_test_split(df_recommendation, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_column = \"id\"\n",
    "user_column = \"reviews_username\"\n",
    "value_column = \"reviews_rating\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the train ratings' dataset into matrix format in which columns are product names and the rows are user names.\n",
    "df_pivot = pd.pivot_table(train,index=user_column, columns = product_column, values = value_column).fillna(0)\n",
    "df_pivot.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dummy train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the train dataset into dummy_train\n",
    "dummy_train = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The products not rated by user is marked as 1 for prediction. \n",
    "dummy_train[value_column] = dummy_train[value_column].apply(lambda x: 0 if x>=1 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the train ratings' dataset into matrix format in which columns are product names and the rows are user names.\n",
    "dummy_train = pd.pivot_table(dummy_train,index=user_column, columns = product_column, values = value_column).fillna(1)\n",
    "dummy_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot.index.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using cosine_similarity function to compute the distance.\n",
    "user_correlation = cosine_similarity(df_pivot)\n",
    "user_correlation[np.isnan(user_correlation)] = 0\n",
    "print(user_correlation)\n",
    "print(user_correlation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering out the user_correlation that are negatively correlated\n",
    "user_correlation[user_correlation<0]=0\n",
    "user_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_predicted_ratings = np.dot(user_correlation, df_pivot.fillna(0))\n",
    "user_predicted_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since we are interested in products that are not rated by the user, we multiply with dummy train to make it zero\n",
    "user_final_rating = np.multiply(user_predicted_ratings,dummy_train)\n",
    "user_final_rating.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find 20 recommendation for the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"02dakota\" \n",
    "print(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = user_final_rating.loc[user_input].sort_values(ascending=False)[0:20]\n",
    "recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the top 20 product id, name and similarity_score \n",
    "final_recommendations = pd.DataFrame({'product_id': recommendations.index, 'similarity_score' : recommendations})\n",
    "final_recommendations.reset_index(drop=True)\n",
    "pd.merge(final_recommendations, train, on=\"id\")[[\"id\", \"name\", \"similarity_score\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation User-User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out the common users of test and train dataset.\n",
    "common = test[test.reviews_username.isin(train.reviews_username)]\n",
    "common.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert into the user-movie matrix.\n",
    "common_user_based_matrix = pd.pivot_table(common,index=user_column, columns = product_column, values = value_column)\n",
    "common_user_based_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the user_correlation matrix into dataframe.\n",
    "user_correlation_df = pd.DataFrame(user_correlation)\n",
    "user_correlation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_correlation_df[user_column] = df_pivot.index\n",
    "user_correlation_df.set_index(user_column,inplace=True)\n",
    "user_correlation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_name = common.reviews_username.tolist()\n",
    "\n",
    "user_correlation_df.columns = df_pivot.index.tolist()\n",
    "user_correlation_df_1 =  user_correlation_df[user_correlation_df.index.isin(list_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_correlation_df_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_correlation_df_2 = user_correlation_df_1.T[user_correlation_df_1.T.index.isin(list_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_correlation_df_3 = user_correlation_df_2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_correlation_df_3[user_correlation_df_3<0]=0\n",
    "\n",
    "common_user_predicted_ratings = np.dot(user_correlation_df_3, common_user_based_matrix.fillna(0))\n",
    "common_user_predicted_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_test = common.copy()\n",
    "\n",
    "dummy_test[value_column] = dummy_test[value_column].apply(lambda x: 1 if x>=1 else 0)\n",
    "dummy_test = pd.pivot_table(dummy_test,index=user_column, columns = product_column, values = value_column).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_user_based_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_user_predicted_ratings = np.multiply(common_user_predicted_ratings,dummy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_user_predicted_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate RMSE\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from numpy import *\n",
    "\n",
    "X  = common_user_predicted_ratings.copy() \n",
    "X = X[X>0]\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(1, 5))\n",
    "print(scaler.fit(X))\n",
    "y = (scaler.transform(X))\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_ = pd.pivot_table(common,index=user_column, columns = product_column, values = value_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding total non-NaN value\n",
    "total_non_nan = np.count_nonzero(~np.isnan(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = (sum(sum((common_ - y )**2))/total_non_nan)**0.5\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item Based Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot = pd.pivot_table(train,\n",
    "    index=product_column,\n",
    "    columns=user_column,\n",
    "    values=value_column\n",
    ")\n",
    "\n",
    "df_pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.nanmean(df_pivot, axis=1)\n",
    "df_subtracted = (df_pivot.T-mean).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subtracted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Item Similarity Matrix\n",
    "item_correlation = 1 - pairwise_distances(df_subtracted.fillna(0), metric='cosine')\n",
    "item_correlation[np.isnan(item_correlation)] = 0\n",
    "print(item_correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_correlation[item_correlation<0]=0\n",
    "item_correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction - item-item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_predicted_ratings = np.dot((df_pivot.fillna(0).T),item_correlation)\n",
    "item_predicted_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering the rating only for the products not rated by the user for recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_final_rating = np.multiply(item_predicted_ratings,dummy_train)\n",
    "item_final_rating.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the top 20 recommendation for the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the user ID as input\n",
    "user_input = '02dakota'\n",
    "print(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommending the Top 5 products to the user.\n",
    "item_recommendations = item_final_rating.loc[user_input].sort_values(ascending=False)[0:20]\n",
    "item_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_final_recommendations = pd.DataFrame({'product_id': item_recommendations.index, 'similarity_score' : item_recommendations})\n",
    "item_final_recommendations.reset_index(drop=True)\n",
    "#final_recommendations.drop(['id'], axis=1)\n",
    "pd.merge(item_final_recommendations, train, on=\"id\")[[\"id\", \"name\", \"similarity_score\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation - item-item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common =  test[test.id.isin(train.id)]\n",
    "common.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_item_based_matrix = common.pivot_table(index=product_column, columns=user_column, values=value_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_correlation_df = pd.DataFrame(item_correlation)\n",
    "item_correlation_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_correlation_df[product_column] = df_subtracted.index\n",
    "item_correlation_df.set_index(product_column,inplace=True)\n",
    "item_correlation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_name = common.id.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_correlation_df.columns = df_subtracted.index.tolist()\n",
    "\n",
    "item_correlation_df_1 =  item_correlation_df[item_correlation_df.index.isin(list_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_correlation_df_2 = item_correlation_df_1.T[item_correlation_df_1.T.index.isin(list_name)]\n",
    "\n",
    "item_correlation_df_3 = item_correlation_df_2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subtracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_correlation_df_3[item_correlation_df_3<0]=0\n",
    "\n",
    "common_item_predicted_ratings = np.dot(item_correlation_df_3, common_item_based_matrix.fillna(0))\n",
    "common_item_predicted_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_test = common.copy()\n",
    "dummy_test[value_column] = dummy_test[value_column].apply(lambda x: 1 if x>=1 else 0)\n",
    "dummy_test = pd.pivot_table(dummy_test, index=product_column, columns=user_column, values=value_column).fillna(0)\n",
    "common_item_predicted_ratings = np.multiply(common_item_predicted_ratings,dummy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_ = pd.pivot_table(common,index=product_column, columns=user_column, values=value_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X  = common_item_predicted_ratings.copy() \n",
    "X = X[X>0]\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(1, 5))\n",
    "print(scaler.fit(X))\n",
    "y = (scaler.transform(X))\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding total non-NaN value\n",
    "total_non_nan = np.count_nonzero(~np.isnan(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = (sum(sum((common_ - y )**2))/total_non_nan)**0.5\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On comparing the RMSE values of User Based Recommender and Item Based Recommender, User based recommendation model seems to be better in this case, as it has a lower RMSE value (~2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the correlation matrix of user based recommender \n",
    "save_object(user_final_rating, \"user_final_rating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Top Product Recommendations - Recommendation of 20 products and filtering by sentiment model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the top 20 product recommendations using the recommender system and get the top 5 using the sentiment ML model.. the similar method would be used in model.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_recommendations(user):\n",
    "    if (user in user_final_rating.index):\n",
    "        # get the product recommedation using the trained ML model\n",
    "        recommendations = list(user_final_rating.loc[user].sort_values(ascending=False)[0:20].index)\n",
    "        temp = df_clean[df_clean.id.isin(recommendations)]\n",
    "        #temp[\"reviews_text_cleaned\"] = temp[\"reviews_text\"].apply(lambda x: self.preprocess_text(x))\n",
    "        #transfor the input data using saved tf-idf vectorizer\n",
    "        X =  tfidf_vectorizer.transform(temp[\"reviews_text_cleaned\"].values.astype(str))\n",
    "        temp[\"predicted_sentiment\"]= xgclf.predict(X)\n",
    "        temp = temp[['name','predicted_sentiment']]\n",
    "        temp_grouped = temp.groupby('name', as_index=False).count()\n",
    "        temp_grouped[\"pos_review_count\"] = temp_grouped.name.apply(lambda x: temp[(temp.name==x) & (temp.predicted_sentiment==1)][\"predicted_sentiment\"].count())\n",
    "        temp_grouped[\"total_review_count\"] = temp_grouped['predicted_sentiment']\n",
    "        temp_grouped['pos_sentiment_percent'] = np.round(temp_grouped[\"pos_review_count\"]/temp_grouped[\"total_review_count\"]*100,2)\n",
    "        return temp_grouped.sort_values('pos_sentiment_percent', ascending=False)\n",
    "    else:\n",
    "        print(f\"User name {user} doesn't exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the above fuction using one of the users that's trained on.\n",
    "get_sentiment_recommendations(\"02dakota\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top 5\n",
    "get_sentiment_recommendations(\"02dakota\")[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the above fuction on the user that doesn't exists or a new user\n",
    "get_sentiment_recommendations(\"nonexistinguser123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample = tfidf_vectorizer.transform([\"Awesome product, will recommend\"])\n",
    "y_pred_sample = xgclf.predict(X_sample)\n",
    "y_pred_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample = tfidf_vectorizer.transform([\"worst product, quality is poor\"])\n",
    "y_pred_sample = xgclf.predict(X_sample)\n",
    "y_pred_sample"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
